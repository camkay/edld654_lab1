---
title: "EDLD654 - Lab 1"
author: "Gluck, Kay, & Miller"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(tidymodels)
library(magrittr)
```

### Read in the `train.csv` data.

```{r, data}
data <- rio::import(here::here("data", "train.csv"))
```

### 1. Initial Split

Split the data into a training set and a testing set as two named objects. Produce the `class` type for the initial split object and the training and test sets.

```{r, initial_split}
set.seed(3000)

data_split <- initial_split(data)

data_train <- training(data_split)
  
data_test  <- testing(data_split)
```

### 2. Use code to show the proportion of the `train.csv` data that went to each of the training and test sets.

```{r}
# proportion of data that went into the test set
nrow(data_train) / nrow(data)

# proportion of data that went into the test set
nrow(data_test) / nrow(data)
```

### 3. *k*-fold cross-validation

Use 10-fold cross-validation to resample the training data.

```{r, resample}
set.seed(3000)

(data_train_r10 <- vfold_cv(data_train))
```

### 4. Use `{purrr}` to add the following columns to your *k*-fold CV object:
* *analysis_n* = the *n* of the analysis set for each fold
* *assessment_n* = the *n* of the assessment set for each fold
* *analysis_p* = the proportion of the analysis set for each fold
* *assessment_p* = the proportion of the assessment set for each fold
* *sped_p* = the proportion of students receiving special education services (`sp_ed_fg`) in the analysis and assessment sets for each fold

```{r, purrr}
# data_train_r10 <- map_dfr(data_train_r10$splits,
#                           function(fold) {
#                             analysis_n     <- nrow(analysis(fold))
#                             assessment_n   <- nrow(assessment(fold))
#                             analysis_p     <- analysis_n / (analysis_n + assessment_n)
#                             assessment_p   <- assessment_n / (analysis_n + assessment_n)
#                             temp           <- table(rbind(assessment(fold),
#                                                           analysis(fold))$sp_ed_fg)
#                             sped_p         <- unname(temp[2] / (temp[1] + temp[2]))
#                             data.frame(analysis_n,
#                                        assessment_n,
#                                        analysis_p,
#                                        assessment_p,
#                                        sped_p)}) %>%
#   bind_cols(data_train_r10, .)

data_train_r10 %<>%
  mutate(analysis_n   = map_dbl(splits, ~nrow(analysis(.x))),
         assessment_n = map_dbl(splits, ~nrow(assessment(.x))),
         analysis_p   = analysis_n / (analysis_n + assessment_n),
         assessment_p = assessment_n / (analysis_n + assessment_n),
         sped_p       = map_dbl(splits, ~prop.table(table(.x$data$sp_ed_fg))[["Y"]]))
```

### 5. Please demonstrate that that there are **no** common values in the `id` columns of the `assessment` data between `Fold01` & `Fold02`, and `Fold09` & `Fold10` (of your 10-fold cross-validation object).

```{r}
# no overlapping values between the two assessment sets across fold 1 and fold 2
length(intersect(assessment(data_train_r10$splits[[1]])$id, assessment(data_train_r10$splits[[2]])$id))

# unique ids is equal to all ideas (suggesting no overlapping values) across fold 1 and fold 2
length(unique(c(assessment(data_train_r10$splits[[1]])$id, assessment(data_train_r10$splits[[2]])$id))) == length(c(assessment(data_train_r10$splits[[1]])$id, assessment(data_train_r10$splits[[2]])$id))

# no overlapping values between the two assessment sets across fold 9 and fold 10
length(intersect(assessment(data_train_r10$splits[[9]])$id, assessment(data_train_r10$splits[[10]])$id))

# unique ids is equal to all ideas (suggesting no overlapping values) across fold 9 and fold 10
length(unique(c(assessment(data_train_r10$splits[[9]])$id, assessment(data_train_r10$splits[[10]])$id))) == length(c(assessment(data_train_r10$splits[[9]])$id, assessment(data_train_r10$splits[[10]])$id))

```

### 6. Try to answer these next questions without running similar code on real data.

For the following code `vfold_cv(fictional_train, v = 20)`:

* What is the proportion in the analysis set for each fold?

.95

* What is the proportion in the assessment set for each fold?

.05

### 7. Use Monte Carlo CV to resample the training data with 20 resamples and .30 of each resample reserved for the assessment sets.

```{r}
set.seed(3000)

data_train_mc <- mc_cv(data_train, prop = .70, times = 20)

```

### 8. Please demonstrate that that there **are** common values in the `id` columns of the `assessment` data between `Resample 8` & `Resample 12`, and `Resample 2` & `Resample 20`in your MC CV object.

```{r}
# there are 12838 overlapping values between resample 8 and resample 12
length(intersect(assessment(data_train_mc$splits[[8]])$id, assessment(data_train_mc$splits[[12]])$id))

# the number of total cases (85,242) is not equal to the number of unique cases (72,404) across resample 8 and resample 12
length(c(assessment(data_train_mc$splits[[8]])$id, assessment(data_train_mc$splits[[12]])$id)) == length(unique(c(assessment(data_train_mc$splits[[8]])$id, assessment(data_train_mc$splits[[12]])$id)))

# there are 12764 overlapping values between resample 2 and resample 20
length(intersect(assessment(data_train_mc$splits[[2]])$id, assessment(data_train_mc$splits[[20]])$id))

# the number of total cases (85,242) is not equal to the number of unique cases (72,478) across resample 2 and resample 20
length(c(assessment(data_train_mc$splits[[2]])$id, assessment(data_train_mc$splits[[20]])$id)) == length(unique(c(assessment(data_train_mc$splits[[2]])$id, assessment(data_train_mc$splits[[20]])$id)))
```

### 9. You plan on doing bootstrap resampling with a training set with *n* = 500.

* What is the sample size of an analysis set for a given bootstrap resample?

500

* What is the sample size of an assessment set for a given bootstrap resample?

Approximately `r (1 - .6321) * 500` (`r (1 - .6321) * 100`%) people will be in the assessment set.

* If each row was selected only once for an analysis set:
  + what would be the size of the analysis set?
500
  + and what would be the size of the assessment set?
0
